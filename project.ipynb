{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datalabframework\n",
    "\n",
    "The datalabframework is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a project\n",
    "\n",
    "One of the main things here is to have configuration and code separated in different files. Project is all about setting the correct working directories where to run and find your notebooks, python files and configuration files. When the datalabframework project is loaded, it starts by searching for a `__main__.py` file, according to python module file naming conventions. When such a file is found, the corresponding directory is set as the root path for the project. All modules and alias paths are all relative to the project root path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the profile can be done with the `datalabframework.project.load` function call. It will look for files ending with `metadata.yml`. The function can optionally set the current working directory and import the key=values of .env file into the python os environment. if no parameters are specified, the default profile is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load in module datalabframework.project:\n",
      "\n",
      "load(profile='default', rootpath=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dlf.project.load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created SparkEngine\n",
      "Init engine \"spark\"\n",
      "Configuring packages:\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  org.postgresql:postgresql:42.2.5\n",
      "Configuring conf:\n",
      "  -  spark.hadoop.fs.s3a.access.key : ****** (redacted)\n",
      "  -  spark.hadoop.fs.s3a.endpoint : http://minio:9000\n",
      "  -  spark.hadoop.fs.s3a.impl : org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "  -  spark.hadoop.fs.s3a.path.style.access : true\n",
      "  -  spark.hadoop.fs.s3a.secret.key : ****** (redacted)\n",
      "Connecting to spark master: local[*]\n",
      "Engine context spark:2.4.1 successfully started\n"
     ]
    }
   ],
   "source": [
    "# Loading default profile\n",
    "import datalabframework as dlf\n",
    "project = dlf.project.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect current project configuration\n",
    "The following will display the configuration of the project metadata profile and configuration data loaded. The configuration is available as a dictionary object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "version: 0.8.1\n",
       "username: jovyan\n",
       "session_name: dlf-tutorial\n",
       "session_id: '0x2985a85c8e5511e9'\n",
       "profile:\n",
       "rootdir: /home/jovyan/work/tutorial\n",
       "script_path: load.ipynb\n",
       "dotenv_path: .env\n",
       "notebooks_files:\n",
       "  - Untitled1.ipynb\n",
       "  - main.ipynb\n",
       "  - install.ipynb\n",
       "  - Untitled2.ipynb\n",
       "  - resources.ipynb\n",
       "  - engine.ipynb\n",
       "  - Untitled.ipynb\n",
       "  - versions.ipynb\n",
       "  - load_compare.ipynb\n",
       "  - metadata.ipynb\n",
       "  - project.ipynb\n",
       "  - scaffolding.ipynb\n",
       "  - join.ipynb\n",
       "  - logging.ipynb\n",
       "  - hello.ipynb\n",
       "  - Untitled3.ipynb\n",
       "  - load.ipynb\n",
       "python_files:\n",
       "  - __main__.py\n",
       "metadata_files:\n",
       "  - metadata.yml\n",
       "repository:\n",
       "    type: git\n",
       "    committer: natbusa\n",
       "    hash: e22c50d\n",
       "    commit: e22c50d90fb7e7d2130c78a47941b997a338951c\n",
       "    branch: master\n",
       "    url: https://github.com/natbusa/dlf-tutorial\n",
       "    name: dlf-tutorial\n",
       "    date: '2019-04-16T18:36:24+07:00'\n",
       "    clean: false"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.project.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a specific profile\n",
    "\n",
    "Loading explicitely a different profile.  \n",
    "In this case the profile `prod` will connect to a cluster in client mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init engine \"spark\"\n",
      "Configuring packages:\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  org.postgresql:postgresql:42.2.5\n",
      "Configuring conf:\n",
      "  -  spark.hadoop.fs.s3a.access.key : ****** (redacted)\n",
      "  -  spark.hadoop.fs.s3a.endpoint : http://minio:9000\n",
      "  -  spark.hadoop.fs.s3a.impl : org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "  -  spark.hadoop.fs.s3a.path.style.access : true\n",
      "  -  spark.hadoop.fs.s3a.secret.key : ****** (redacted)\n",
      "Connecting to spark master: spark://spark-master:7077\n",
      "Engine context spark:2.4.1 successfully started\n"
     ]
    }
   ],
   "source": [
    "# Loading default profile\n",
    "project = dlf.project.load('prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
