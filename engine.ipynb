{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datalabframework\n",
    "\n",
    "The datalabframework is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the engine\n",
    "\n",
    "Super simple, yet flexible :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created SparkEngine\n",
      "Init engine \"spark\"\n",
      "Connecting to spark master: local[*]\n",
      "Engine context spark:2.4.1 successfully started\n"
     ]
    }
   ],
   "source": [
    "#start the engine\n",
    "engine = dlf.engine('spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can also use directlt the specific engine class\n",
    "engine = dlf.SparkEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and saving data resources is an operation performed by the engine. The engine configuration can be passed straight as parameters in the engine call, or configured in metadata yaml files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine Context\n",
    "\n",
    "You can access the underlying engine by referring to the engine.context. In particular for the spark engine the context can be accessed with the next example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = dlf.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://72ec1747fca0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>None</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa47ea2ac18>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|yes|  1|\n",
      "| no|  0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe with two columns, named resp. 'a' and 'b'\n",
    "\n",
    "df = spark.createDataFrame([('yes',1),('no',0)], ('a', 'b'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dlf.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.driver.host': '72ec1747fca0',\n",
       " 'spark.rdd.compress': 'True',\n",
       " 'spark.serializer.objectStreamReset': '100',\n",
       " 'spark.master': 'local[*]',\n",
       " 'spark.app.id': 'local-1561007168566',\n",
       " 'spark.executor.id': 'driver',\n",
       " 'spark.submit.deployMode': 'client',\n",
       " 'spark.app.name': 'None',\n",
       " 'spark.driver.port': '35319',\n",
       " 'spark.ui.showConsoleProgress': 'true'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SPARK_HOME: /opt/spark\n",
       "HADOOP_HOME: /opt/hadoop\n",
       "JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64\n",
       "PYSPARK_PYTHON: /opt/conda/bin/python\n",
       "PYSPARK_DRIVER_PYTHON: /opt/conda/bin/python\n",
       "PYTHONPATH: /opt/spark/python:/opt/spark/python/lib/py4j-0.10.7-src.zip\n",
       "PYSPARK_SUBMIT_ARGS: ' pyspark-shell'\n",
       "SPARK_DIST_CLASSPATH:"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full configuration, please uncomment and execute the following statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_version': '3.6.3',\n",
       " 'hadoop_version': '3.1.1',\n",
       " 'hadoop_detect': 'spark',\n",
       " 'hadoop_home': '/opt/hadoop',\n",
       " 'spark_home': '/opt/spark',\n",
       " 'spark_classpath': ['/opt/spark/jars/*',\n",
       "  '/opt/hadoop/etc/hadoop',\n",
       "  '/opt/hadoop/share/hadoop/common/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/common/*',\n",
       "  '/opt/hadoop/share/hadoop/hdfs',\n",
       "  '/opt/hadoop/share/hadoop/hdfs/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/hdfs/*',\n",
       "  '/opt/hadoop/share/hadoop/mapreduce/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/mapreduce/*',\n",
       "  '/opt/hadoop/share/hadoop/yarn',\n",
       "  '/opt/hadoop/share/hadoop/yarn/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/yarn/*'],\n",
       " 'spark_classpath_source': '/opt/spark/conf/spark-env.sh'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting engine parameters during engine initalization\n",
    "\n",
    "Submit master, configuration parameters and services as engine params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init engine \"spark\"\n",
      "Configuring packages:\n",
      "  -  org.postgresql:postgresql:42.2.5\n",
      "Connecting to spark master: spark://spark-master:7077\n",
      "Engine context spark:2.4.1 successfully started\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datalabframework.spark.engine.SparkEngine at 0x7fa47ec80cf8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "dlf.engine('spark', master='spark://spark-master:7077', services='postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.driver.host': '72ec1747fca0',\n",
       " 'spark.driver.port': '40205',\n",
       " 'spark.executor.id': 'driver',\n",
       " 'spark.app.id': 'app-20190620050618-0000',\n",
       " 'spark.submit.pyFiles': '/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.jars': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.rdd.compress': 'True',\n",
       " 'spark.master': 'spark://spark-master:7077',\n",
       " 'spark.repl.local.jars': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.serializer.objectStreamReset': '100',\n",
       " 'spark.submit.deployMode': 'client',\n",
       " 'spark.app.name': 'None',\n",
       " 'spark.files': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.ui.showConsoleProgress': 'true'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.engine().conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
