{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datalabframework\n",
    "\n",
    "The datalabframework is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles\n",
    "\n",
    "- Both notebooks and code files are first citizens\n",
    "\n",
    "Following python package conventions, the root of the project is tagged by a `__main__.py` and directory of source code (either python or notebooks) contains the `__init__.py` code. By doing so, python and notebook files can reference each other.\n",
    "\n",
    "Python notebooks and Python code can be mixed and matched, and are interoperable with each other. By using the datalabframework, you can include notebooks as modules to python code, and you can include python modules in a notebook. \n",
    "\n",
    "- Decouple Code and Data Resources\n",
    "\n",
    "Data can be located anywhere, on remote HDFS clusters, or Object Store Services exposed via S3 protocols etc. Also you can keep data on the local file system. No matter where data is located, we want to de-couple data resources from the code executed in the data pipeline.\n",
    "\n",
    "Separating data and code is done by declaring all data resources/providers as configuration in metadata files. Metadata files make possible to define aliases for data resources, data services and engine configurations, and keeping the ETL and ML code tidy with no hardcoded parameters.\n",
    "\n",
    "- Decouple Code from Configuration\n",
    "\n",
    "Code either stored as notebooks or as python files should be decoupled from both engine configurations and from data locations. All configuration is kept in `metadata.yml` yaml files. Multiple setups for test, exploration, production can be described  in the same `metadata.yml` file or in separate multiple files using __profiles__. All profiles inherit from a default profiles, to reduce duplication of configurations settings across profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapters\n",
    "\n",
    "  - [install.ipynb](Installing)\n",
    "  - [load.ipynb](Load a project)\n",
    "  - [metadata.ipynb](Configuring Metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lab Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package things\n",
    "Package version: package variables `version_info`, `__version__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 7, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check is the datalabframework is loaded in the current python context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the datalabframework is loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    __DATALABFRAMEWORK__\n",
    "    print(\"the datalabframework is loaded\")\n",
    "except NameError:\n",
    "    print(\"the datalabframework is not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logging', 'project']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of modules loaded as `from datalabframework import * ` \n",
    "dlf.__all__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules: project\n",
    "\n",
    "Project is all about setting the correct working directories where to run and find your notebooks, python files and configuration files. When the datalabframework is imported, it starts by searching for a `__main__.py` file, according to python module file naming conventions. All modules and alias paths are all relative to this project root path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a project profile\n",
    "\n",
    "Loading the profile can be done with the `datalabframework.project.load` function call. It will look for files ending with `metadata.yml`. The function can optionally set the current working directory and import the key=values of .env file into the python os environment. if no parameters are specified, the default profile is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load in module datalabframework.project:\n",
      "\n",
      "load(profile='default', rootdir_path=None, search_parent_dirs=True, factory_defaults=True)\n",
      "    Performs the following steps:\n",
      "        - set rootdir for the given project\n",
      "        - import variables from  <rootdir>/.env (if present),\n",
      "        - load the `profile` from the metadata files\n",
      "        - setup and start the data engine\n",
      "    \n",
      "    :param profile: load the given metadata profile (default: 'default')\n",
      "    :param rootdir_path: root directory for loaded project (default: current working directory)\n",
      "    \n",
      "    :param search_parent_dirs: search parent dirs to detect rootdir by \n",
      "           looking for a '__main__.py' or 'main.ipynb' file (default: True)\n",
      "    \n",
      "    :param factory_defaults: add preset default configuration. \n",
      "           Project provided metadata file can override this default values (default: True)\n",
      "    \n",
      "    :return: None\n",
      "    \n",
      "    Note that:\n",
      "    \n",
      "    1)  Metadata files are merged up, so you can split the information in multiple \n",
      "        files as long as they end with `metadata.yml`. \n",
      "    \n",
      "        For example: `metadata.yml`, `abc.metadata.yaml`, `abc_metadata.yml` \n",
      "        are all valid metadata file names.\n",
      "    \n",
      "    2)  All metadata files in all subdirectories from the project root directory are loaded,\n",
      "        unless the directory contains a file `metadata.ignore.yml`\n",
      "    \n",
      "    3)  Metadata files can provide multiple profile configurations,\n",
      "        by separating each profile configuration with a Document Marker ( a line with `---`)\n",
      "        (see https://yaml.org/spec/1.2/spec.html#YAML)\n",
      "    \n",
      "    4)  Each metadata profile, can be broken down in multiple yaml files,\n",
      "        When loading the files all configuration belonging to the same profile with be merged.\n",
      "    \n",
      "    5)  All metadata profiles inherit the settings from profile 'default'\n",
      "    \n",
      "    6)  If `factory_defaults` is set to true, \n",
      "        the provided metadata profiles will inherits from a factory defaults file set as:\n",
      "         ```\n",
      "            %YAML 1.2\n",
      "            ---\n",
      "            profile: default\n",
      "            variables: {}\n",
      "            engine:\n",
      "                type: spark\n",
      "                master: local[*]\n",
      "            providers: {}\n",
      "            resources: {}\n",
      "            loggers:\n",
      "                root:\n",
      "                    severity: info\n",
      "                datalabframework:\n",
      "                    name: dlf\n",
      "                    stream:\n",
      "                        enable: true\n",
      "                        severity: notice\n",
      "            ---\n",
      "            profile: prod\n",
      "            ---\n",
      "            profile: stage\n",
      "            ---\n",
      "            profile: test\n",
      "            ---\n",
      "            profile: dev\n",
      "    \n",
      "         ```\n",
      "    \n",
      "    Metadata files are composed of 6 sections:\n",
      "        - profile\n",
      "        - variables\n",
      "        - providers\n",
      "        - resources\n",
      "        - engine\n",
      "        - loggers\n",
      "    \n",
      "    For more information about metadata configuration,\n",
      "    type `help(datalabframework.project.metadata)`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dlf.project.load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function metadata in module datalabframework.project:\n",
      "\n",
      "metadata()\n",
      "    return a metadata object which provides just one method:\n",
      "    :return: a Metadata object\n",
      "    \n",
      "    Notes about metadata configurations:\n",
      "    \n",
      "    #### Metadata files\n",
      "    \n",
      "    1) Metadata files are merged up, so you can split the information in \n",
      "       multiple files as long as they end with `metadata.yml`.   \n",
      "       For example: `metadata.yml`, `abc.metadata.yaml`, `abc_metadata.yml` \n",
      "       are all valid metadata file names.\n",
      "    \n",
      "    2) All metadata files in all subdirectories from the project root directory are loaded,\n",
      "       unless the directory contains a file `metadata.ignore.yml`\n",
      "    \n",
      "    3) Metadata files can provide multiple profile configurations, \n",
      "       by separating each profile configuration with as `bare documents` wihtin the same yaml file  \n",
      "       by separating the configuration with a line containing three hyphens (`---`)  \n",
      "       (see https://yaml.org/spec/1.2/spec.html#YAML)\n",
      "    \n",
      "    4) Each metadata profile, can be broken down in multiple yaml files,  \n",
      "       When loading the files all configuration belonging to the same profile with be merged. \n",
      "    \n",
      "    5) All metadata profiles inherit the settings from profile 'default'\n",
      "    \n",
      "    6)  If `factory_defaults` is set to true, when a project metadata is loaded\n",
      "        the provided metadata profiles will extend/override the following metadata configuration:\n",
      "         ```\n",
      "            %YAML 1.2\n",
      "            ---\n",
      "            profile: default\n",
      "            variables: {}\n",
      "            engine:\n",
      "                type: spark\n",
      "                master: local[*]\n",
      "            providers: {}\n",
      "            resources: {}\n",
      "            loggers:\n",
      "                root:\n",
      "                    severity: info\n",
      "                datalabframework:\n",
      "                    name: dlf\n",
      "                    stream:\n",
      "                        enable: true\n",
      "                        severity: notice\n",
      "            ---\n",
      "            profile: prod\n",
      "            ---\n",
      "            profile: stage\n",
      "            ---\n",
      "            profile: test\n",
      "            ---\n",
      "            profile: dev\n",
      "    \n",
      "         ```\n",
      "    \n",
      "    \n",
      "    #### Metadata sections\n",
      "    \n",
      "    Metadata files are composed of 6 sections:\n",
      "        - profile \n",
      "        - variables\n",
      "        - providers \n",
      "        - resources\n",
      "        - engine\n",
      "        - loggers\n",
      "    \n",
      "    ##### Profile name\n",
      "    A metadata configuration supports multiple profiles. \n",
      "    By default the following profiles are present in the configuration, \n",
      "    when loading the metadata with the argument `factory_defaults=True`\n",
      "    \n",
      "     - default\n",
      "     - prod\n",
      "     - stage\n",
      "     - test\n",
      "     - test\n",
      "    \n",
      "    You can define and use the custom profiles. \n",
      "    by loading a different profile you can define different configuratioon for your data resources, \n",
      "    without having to modify your code. For instance, you cat setup the files to be saves on local \n",
      "    disk for testing and in hdfs for production, as described in this snippet below:\n",
      "    \n",
      "    ```\n",
      "        ---\n",
      "        profile: default\n",
      "        providers:\n",
      "            processed_data:\n",
      "                service: local\n",
      "                path: data\n",
      "                format: parquet\n",
      "        ---\n",
      "        profile: prod\n",
      "        providers:\n",
      "            processed_data:\n",
      "                service: hdfs\n",
      "                hostname: hdfs-namenode\n",
      "                path: /prod/data\n",
      "        ---\n",
      "        profile: test\n",
      "    ```\n",
      "    \n",
      "    In the above example, the profiles `test` and `default` share the same configuration,   \n",
      "    while the profile `prod` defined the provider alias `processed_data` as an hdfs location.\n",
      "    \n",
      "    You can also use profiles to define different options configurations for the spark engine\n",
      "    or different logging options. Here below an example of a default configuration which uses \n",
      "    a local spark setup in test/dev while using a spark cluster for prod and stage profiles\n",
      "    \n",
      "    ```\n",
      "        ---\n",
      "        profile: default\n",
      "        engine:\n",
      "            type: spark\n",
      "            master: local[*]\n",
      "        ---\n",
      "        profile: prod\n",
      "        engine:\n",
      "            type: spark\n",
      "            master: spark://spark-prod-cluster:17077\n",
      "        ---\n",
      "        profile: stage\n",
      "        engine:\n",
      "            type: spark\n",
      "            master: spark://spark-stage-cluster:17077\n",
      "        ---\n",
      "        profile: test\n",
      "    ```\n",
      "    \n",
      "    ##### Profile Variables\n",
      "    \n",
      "    The variable section in the profile allows you to compose information and \n",
      "    reuse them in other part of the configuration.  The datalabframework yaml files \n",
      "    support jinja2 templates for variable substitution. The template rendering \n",
      "    is only performed once upon project load.\n",
      "    \n",
      "    Here below an example of a variable section and \n",
      "    how to use it for the rest of the configuration:\n",
      "    ```\n",
      "        ---\n",
      "        profile: default\n",
      "        variables: \n",
      "          a: hello\n",
      "          b: \"{{ variables.a}} world\"\n",
      "          c: \"{{ env('SHELL') }}\"\n",
      "          d: \"{{ env('ENV_VAR_NOT_DEFINED', 'foo'}}\"\n",
      "          e: \"{{ now() }}\"\n",
      "          f: \"{{ now(tz='UTC', format='%Y-%m-%d %H:%M:%S') }}\"\n",
      "    \n",
      "          my_string_var: \"Hi There!\"\n",
      "          my_env_var: \"{{ env('DB_USERNAME', 'guest') }}\"\n",
      "          my_concat_var: \"{{ engine.type }} running at {{ engine.master }}\"\n",
      "    \n",
      "        ---\n",
      "    ```\n",
      "    \n",
      "    The above metadata profile will be rendered as:\n",
      "    \n",
      "    ```\n",
      "        ---\n",
      "        profile: default\n",
      "        variables:\n",
      "            a: hello\n",
      "            b: hello world\n",
      "            c: /bin/bash\n",
      "            d: foo\n",
      "            e: '2019-03-27 08:42:00'\n",
      "            f: '2019-03-27'\n",
      "            my_string_var: Hi There!\n",
      "            my_env_var: guest\n",
      "            my_concat_var: spark running at local[*]\n",
      "        ---\n",
      "    ```\n",
      "    \n",
      "    Note that:\n",
      "    \n",
      "     - variables can be defined in multiple profiles\n",
      "    \n",
      "     - variables section in a give profile always  \n",
      "       inherit the variable from the `default` profile\n",
      "    \n",
      "     - a maximum of 5 rendering passes if allowed\n",
      "    \n",
      "     - values including a jinja template context must alwasy be quoted.  \n",
      "       As is my_var: \"{{ <my_jinja-template > }}\"\n",
      "    \n",
      "    Accessing configuration values in a jinja template:\n",
      "    \n",
      "    Yaml object values can be referenced in the jinja template using the . notation.\n",
      "    To access the data item, provide the path from the root of the profile. \n",
      "    For instance the provider `processed_data` format in the example above can be referenced as: \n",
      "        `providers.processed_data.format`\n",
      "    \n",
      "    Jinja rendering operations:\n",
      "    Please refer to <url> for a list of operators on jinja variables\n",
      "    \n",
      "    Metadata Jinja functions:\n",
      "    On top of the default set of operations, \n",
      "    two functions can be used inside a jinja rendering context:\n",
      "    \n",
      "    `def env(env_var, default_value='null')`\n",
      "    renders in the template the value of environment variable `env_var` or null if not available.\n",
      "    This function can be useful (also in combination with a .env file), to avoid hard-coding \n",
      "    passwords and other login/auth data in the metadata configuration. Note that is setup is meant\n",
      "    for convinence and not for security. Example:\n",
      "    \n",
      "        my_env_var: \"{{ env('DB_USERNAME', 'guest') }}\"\n",
      "    \n",
      "    `def now(tz='UTC', format='%Y-%m-%d %H:%M:%S')`\n",
      "    renders the system current datetime value, optionally a different timezone and string formatting\n",
      "    option can be added. This function can be useul if you want to execute code on a time window \n",
      "    relative to the current time. Example:\n",
      "    \n",
      "        utc_now: \"{{ now(tz='UTC', format='%Y-%m-%d %H:%M:%S') }}\"\n",
      "    \n",
      "    \n",
      "    ##### Profile Providers\n",
      "    \n",
      "    A provider is a service which allows you to load and save data. The datalabframework \n",
      "    extend the spark load save API calls by decoupling the provider configuration from the code.\n",
      "    The `providers` section in the metadata allow you to define a arbitrary number of providers. \n",
      "    A provider are declared as an alias defining a set of properties. See example below:\n",
      "    \n",
      "    ```\n",
      "        profile: default\n",
      "    \n",
      "        providers:\n",
      "            my_provider:\n",
      "                service: hdfs\n",
      "                hostname: hdfs-namenode\n",
      "                path: /foo\n",
      "                format: parquet\n",
      "    ```\n",
      "    \n",
      "    This table provides a list of valid properties you can defined for a given provider alias:\n",
      "    \n",
      "    `service`:  \n",
      "        The service which is going to be use for load/save data. \n",
      "        Supported services:\n",
      "          - minio\n",
      "          - hdfs\n",
      "          - local\n",
      "          - mysql\n",
      "          - postgres\n",
      "          - oracle\n",
      "          - mssql\n",
      "          - sqlite\n",
      "    \n",
      "    `format`:  \n",
      "        The format used for reading and writing data\n",
      "        Default is 'parquet' for all filesystem and object store services.\n",
      "        For other type of services, such as databases, this property is ignored.\n",
      "        Supported formats: \n",
      "          - jdbc\n",
      "          - nosql\n",
      "          - csv\n",
      "          - parquet\n",
      "          - json\n",
      "          - jsonl\n",
      "    \n",
      "    `host`, `hostname`:   \n",
      "    The name of the host providing the service\n",
      "    Default is 127.0.0.1\n",
      "    \n",
      "    `port`:   \n",
      "    The port number of the host providing the service\n",
      "    The default port depends on the service according to the following table:\n",
      "    \n",
      "        - hdfs: 8020\n",
      "        - mysql: 3306\n",
      "        - postgres: 5432\n",
      "        - mssql: 1433\n",
      "        - oracle: 1521\n",
      "        - elastic: 9200\n",
      "        - minio:9000\n",
      "    \n",
      "    `database`:  \n",
      "    The database name from the selected jdbc service\n",
      "    \n",
      "    `path`:  \n",
      "    The root path used to save/load data resources. If the path is a fully qualified url\n",
      "    such as (`hdfs://data.cluster.local/foo/bar`), it will be used straight away.\n",
      "    \n",
      "    Otherwise the url will be assembled using the following properties:\n",
      "       - `service`\n",
      "       - `host`\n",
      "       - `port`\n",
      "       - `database`\n",
      "       - `path`\n",
      "    \n",
      "    When the service is a jdbc connection, \n",
      "    the property `path` of the provider can be used to refer to the database name\n",
      "    \n",
      "    `username`,  \n",
      "    `password`,  \n",
      "    The credential for authenticating for the given providers\n",
      "    \n",
      "    `cache`:  \n",
      "    Cache the data, before saving or after loading\n",
      "    \n",
      "    `date_column`:   \n",
      "    define a column in the dataframe to be the date column (for faster read/write)  \n",
      "    \n",
      "    `date_start`:  \n",
      "    filter data according to this start date/datetime for the `date_column`\n",
      "    \n",
      "    `date_end`:  \n",
      "    filter data according to this end date/datetime for the `date_column`\n",
      "    \n",
      "    `date_window`:  \n",
      "    in combination with either `date_end` or `date_start` \n",
      "    it defines a filter interval for the `date_column`. \n",
      "    If defined and valid, this is implicitely applied when loading and saving data.\n",
      "    \n",
      "    `date_partition`:  \n",
      "    `update_column`:  \n",
      "    `hash_column`:  \n",
      "    `state_column`:  \n",
      "    `hash_column`:  \n",
      "    Add special columns to the dataframe.\n",
      "    \n",
      "    `options`:  \n",
      "    Extra options, as defined in the selected engine for load/save\n",
      "    \n",
      "    ##### Profile Resources\n",
      "    \n",
      "    Resources are defined in the same way as providers, with the difference that\n",
      "    but you can specify a provider parameter. In this case the resource inherits\n",
      "    the provider's configuration. By sharing most of the configuration for the resources as \n",
      "    common provider configuration, you can keep resource configuration very lean and coincise.\n",
      "    \n",
      "    `path`\n",
      "    When the service is a jdbc connection, \n",
      "    the property `path` of a resource can be used to refer to the table name\n",
      "    \n",
      "    When the service is a file system or block storage or an object store,\n",
      "    The reource path is appended/concatenated to the provider's path\n",
      "    \n",
      "    `provider`:\n",
      "    The provider alias configuration, which is used as base configuration fro this resource.\n",
      "    \n",
      "    \n",
      "    ##### Profile Engine\n",
      "    \n",
      "    This section defines the engine configurations to process data.\n",
      "    The following properties can be defined:\n",
      "    \n",
      "    `type`:  \n",
      "    Engine type. Currently supports is limited to the option `spark`\n",
      "    \n",
      "    `master`:\n",
      "    The url of the spark master (e.g. `spark://23.195.26.187:7077`)\n",
      "    \n",
      "    `jobname`:  \n",
      "    The name of the application to be reported in the spark-ui and history server\n",
      "    Default is constructed using the git repository name (if available) and the profile name\n",
      "    \n",
      "    `timezone`:\n",
      "    This option allows spark to interpret the datetime data as belonging \n",
      "    to a different timezone than the one provided by the machine defaults.\n",
      "    \n",
      "    When set to `naive` will interpret each datetime object as 'naive', \n",
      "    no timezone translation will be executed. This option is equivalent \n",
      "    to setting the `timezone` parameter to 'UTC'\n",
      "    \n",
      "    `submit`:  \n",
      "    A section which allows to definea and add a number of files during the engine's initalization.\n",
      "    Files are declared as belonging to the following groups\n",
      "    \n",
      "     - `jars`\n",
      "     - `packages`\n",
      "     - `py-files`\n",
      "    \n",
      "    `config`:  \n",
      "     A list of custom configurations, defined as key, value pairs.\n",
      "     For example, check out the list of valid Spark 2.4.0 configurations as provided at  \n",
      "     https://spark.apache.org/docs/2.4.0/configuration.html\n",
      "    \n",
      "    ##### Profile Logging\n",
      "    This section define the logging configuration.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dlf.project.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'profile': 'default', 'variables': {'a': 'hello', 'b': 'hello world', 'c': '/bin/bash', 'd': 'foo', 'e': '2019-04-11 03:36:52', 'f': '2019-04-11', 'my_date_var': '2019-04-11', 'my_env_var': 'guest', 'my_string_concat_var1': 'spark running at local[*]', 'my_string_concat_var2': 'Hi There!: the current date is 2019-04-11', 'my_string_var': 'Hi There!'}, 'engine': {'type': 'spark', 'master': 'local[*]', 'jobname': None, 'timezone': 'naive', 'submit': {'jars': None, 'packages': None, 'py-files': None, '': None}, 'config': {'spark.sql.autoBroadcastJoinThreshold': -1}}, 'providers': {'hdfs': {'format': 'parquet', 'path': '/', 'service': 'hdfs', 'hostname': 'hdfs-namenode'}, 'minio': {'format': 'parquet', 'password': 'minio-password', 'username': 'minio-username', 'path': '/data', 'service': 'minio', 'hostname': 'minio'}, 'pagila': {'format': 'jdbc', 'password': 'postgres', 'username': 'postgres', 'path': 'pagila', 'service': 'postgres', 'hostname': 'postgres'}, 'sakila': {'format': 'jdbc', 'password': 'None', 'username': 'mysql', 'path': 'sakila', 'service': 'mysql', 'hostname': 'mysql'}, 'localfs': {'path': 'data', 'service': 'file', 'format': 'csv'}}, 'resources': {'ascombe': {'path': 'ascombe.csv', 'provider': 'localfs', 'options': {'inferSchema': True, 'header': True}}, 'correlation': {'path': 'correlation.csv', 'provider': 'localfs'}}, 'loggers': {'root': {'severity': 'info'}, 'datalabframework': {'name': 'dlf', 'stream': {'severity': None, 'enable': None}, 'stdio': {'severity': 'notice', 'enable': True}, 'file': {'severity': 'notice', 'enable': True, 'path': None}, 'kafka': {'severity': 'info', 'enable': False, 'hosts': 'kafka-node1:9092 kafka-node2:9092', 'topic': 'dlf'}}}}\n",
      "Loading packages:\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.postgresql:postgresql:42.2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "type: spark\n",
       "name: default\n",
       "version: 2.4.0\n",
       "info:\n",
       "    python_version: 3.6.8\n",
       "    hadoop_version: 3.1.1\n",
       "    hadoop_detect: spark\n",
       "    hadoop_home: /opt/hadoop\n",
       "    spark_home: /opt/spark\n",
       "    spark_classpath:\n",
       "      - /opt/spark/jars/*\n",
       "      - /opt/hadoop/etc/hadoop\n",
       "      - /opt/hadoop/share/hadoop/common/lib/*\n",
       "      - /opt/hadoop/share/hadoop/common/*\n",
       "      - /opt/hadoop/share/hadoop/hdfs\n",
       "      - /opt/hadoop/share/hadoop/hdfs/lib/*\n",
       "      - /opt/hadoop/share/hadoop/hdfs/*\n",
       "      - /opt/hadoop/share/hadoop/mapreduce/lib/*\n",
       "      - /opt/hadoop/share/hadoop/mapreduce/*\n",
       "      - /opt/hadoop/share/hadoop/yarn\n",
       "      - /opt/hadoop/share/hadoop/yarn/lib/*\n",
       "      - /opt/hadoop/share/hadoop/yarn/*\n",
       "    spark_classpath_source: /opt/spark/conf/spark-env.sh\n",
       "config:\n",
       "    spark.driver.host: 5de54992bc62\n",
       "    spark.repl.local.jars: file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.hadoop.fs.s3a.path.style.access: 'true'\n",
       "    spark.jars: file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.executor.extraJavaOptions: -Duser.timezone=UTC\n",
       "    spark.hadoop.fs.s3a.access.key: minio-username\n",
       "    spark.executor.id: driver\n",
       "    spark.submit.pyFiles: /home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,/home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,/home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,/home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.driver.port: '43027'\n",
       "    spark.hadoop.fs.s3a.secret.key: minio-password\n",
       "    spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "    spark.driver.extraJavaOptions: -Duser.timezone=UTC\n",
       "    spark.rdd.compress: 'True'\n",
       "    spark.files: file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.serializer.objectStreamReset: '100'\n",
       "    spark.master: local[*]\n",
       "    spark.app.id: local-1554953819310\n",
       "    spark.submit.deployMode: client\n",
       "    spark.hadoop.fs.s3a.endpoint: http://minio:9000\n",
       "    spark.sql.session.timeZone: UTC\n",
       "    spark.sql.autoBroadcastJoinThreshold: '-1'\n",
       "    spark.app.name: None\n",
       "    spark.ui.showConsoleProgress: 'true'\n",
       "env:\n",
       "    SPARK_HOME: /opt/spark\n",
       "    HADOOP_HOME: /opt/hadoop\n",
       "    JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64\n",
       "    PYTHONPATH: /opt/spark/python:/opt/spark/python/lib/py4j-0.10.7-src.zip\n",
       "    PYSPARK_SUBMIT_ARGS: ' --packages org.apache.hadoop:hadoop-aws:3.1.1,mysql:mysql-connector-java:8.0.12,org.postgresql:postgresql:42.2.5\n",
       "        pyspark-shell'\n",
       "    SPARK_DIST_CLASSPATH:\n",
       "rootdir: /home/jovyan/work/basic\n",
       "timezone: UTC"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "\n",
    "# Loading default profile\n",
    "project = dlf.project.load()\n",
    "dlf.project.engine().config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTICE - run_code - ss - {'l': 11}\n",
      "WARNING - run_code - data - {'l': 11, 'a': 42}\n",
      "WARNING - run_code - data - {'l': 11}\n",
      "WARNING - run_code - hello - {}\n"
     ]
    }
   ],
   "source": [
    "dlf.logging.notice('ss', extra={'l':11})\n",
    "dlf.logging.warning({'l':11}, extra={'a':42})\n",
    "dlf.logging.warning({'l':11})\n",
    "dlf.logging.warning('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading packages:\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.postgresql:postgresql:42.2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "profile: default\n",
       "variables:\n",
       "    a: hello\n",
       "    b: hello world\n",
       "    c: /bin/bash\n",
       "    d: foo\n",
       "    e: '2019-04-11 03:40:07'\n",
       "    f: '2019-04-11'\n",
       "    my_date_var: '2019-04-11'\n",
       "    my_env_var: guest\n",
       "    my_string_concat_var1: spark running at local[*]\n",
       "    my_string_concat_var2: 'Hi There!: the current date is 2019-04-11'\n",
       "    my_string_var: Hi There!\n",
       "engine:\n",
       "    type: spark\n",
       "    master: local[*]\n",
       "    jobname:\n",
       "    timezone: naive\n",
       "    submit:\n",
       "        jars:\n",
       "        packages:\n",
       "        py-files:\n",
       "    config:\n",
       "        spark.sql.autoBroadcastJoinThreshold: -1\n",
       "providers:\n",
       "    pagila:\n",
       "        hostname: postgres\n",
       "        username: postgres\n",
       "        service: postgres\n",
       "        format: jdbc\n",
       "        path: pagila\n",
       "        password: postgres\n",
       "    hdfs:\n",
       "        hostname: hdfs-namenode\n",
       "        service: hdfs\n",
       "        format: parquet\n",
       "        path: /\n",
       "    localfs:\n",
       "        service: file\n",
       "        format: csv\n",
       "        path: data\n",
       "    sakila:\n",
       "        hostname: mysql\n",
       "        username: mysql\n",
       "        service: mysql\n",
       "        format: jdbc\n",
       "        path: sakila\n",
       "        password: None\n",
       "    minio:\n",
       "        hostname: minio\n",
       "        username: minio-username\n",
       "        service: minio\n",
       "        format: parquet\n",
       "        path: /data\n",
       "        password: minio-password\n",
       "resources:\n",
       "    correlation:\n",
       "        provider: localfs\n",
       "        path: correlation.csv\n",
       "    ascombe:\n",
       "        options:\n",
       "            header: true\n",
       "            inferSchema: true\n",
       "        provider: localfs\n",
       "        path: ascombe.csv\n",
       "loggers:\n",
       "    root:\n",
       "        severity: info\n",
       "    datalabframework:\n",
       "        name: dlf\n",
       "        stream:\n",
       "            severity:\n",
       "            enable:\n",
       "        stdio:\n",
       "            severity: notice\n",
       "            enable: true\n",
       "        file:\n",
       "            severity: notice\n",
       "            enable: true\n",
       "            path:\n",
       "        kafka:\n",
       "            severity: info\n",
       "            enable: false\n",
       "            hosts: kafka-node1:9092 kafka-node2:9092\n",
       "            topic: dlf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "\n",
    "# Loading default profile\n",
    "project = dlf.project.load()\n",
    "dlf.project.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type: spark\n",
       "name: default\n",
       "version: 2.4.0\n",
       "info:\n",
       "    python_version: 3.6.8\n",
       "    hadoop_version: 3.1.1\n",
       "    hadoop_detect: spark\n",
       "    hadoop_home: /opt/hadoop\n",
       "    spark_home: /opt/spark\n",
       "    spark_classpath:\n",
       "      - /opt/spark/jars/*\n",
       "      - /opt/hadoop/etc/hadoop\n",
       "      - /opt/hadoop/share/hadoop/common/lib/*\n",
       "      - /opt/hadoop/share/hadoop/common/*\n",
       "      - /opt/hadoop/share/hadoop/hdfs\n",
       "      - /opt/hadoop/share/hadoop/hdfs/lib/*\n",
       "      - /opt/hadoop/share/hadoop/hdfs/*\n",
       "      - /opt/hadoop/share/hadoop/mapreduce/lib/*\n",
       "      - /opt/hadoop/share/hadoop/mapreduce/*\n",
       "      - /opt/hadoop/share/hadoop/yarn\n",
       "      - /opt/hadoop/share/hadoop/yarn/lib/*\n",
       "      - /opt/hadoop/share/hadoop/yarn/*\n",
       "    spark_classpath_source: /opt/spark/conf/spark-env.sh\n",
       "config:\n",
       "    spark.driver.host: 5de54992bc62\n",
       "    spark.repl.local.jars: file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.app.id: local-1554954011735\n",
       "    spark.hadoop.fs.s3a.path.style.access: 'true'\n",
       "    spark.jars: file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.executor.extraJavaOptions: -Duser.timezone=UTC\n",
       "    spark.hadoop.fs.s3a.access.key: minio-username\n",
       "    spark.executor.id: driver\n",
       "    spark.submit.pyFiles: /home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,/home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,/home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,/home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.hadoop.fs.s3a.secret.key: minio-password\n",
       "    spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "    spark.driver.extraJavaOptions: -Duser.timezone=UTC\n",
       "    spark.rdd.compress: 'True'\n",
       "    spark.files: file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.serializer.objectStreamReset: '100'\n",
       "    spark.master: local[*]\n",
       "    spark.submit.deployMode: client\n",
       "    spark.hadoop.fs.s3a.endpoint: http://minio:9000\n",
       "    spark.driver.port: '36467'\n",
       "    spark.sql.session.timeZone: UTC\n",
       "    spark.app.name: None\n",
       "    spark.sql.autoBroadcastJoinThreshold: '-1'\n",
       "    spark.ui.showConsoleProgress: 'true'\n",
       "env:\n",
       "    SPARK_HOME: /opt/spark\n",
       "    HADOOP_HOME: /opt/hadoop\n",
       "    JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64\n",
       "    PYTHONPATH: /opt/spark/python:/opt/spark/python/lib/py4j-0.10.7-src.zip\n",
       "    PYSPARK_SUBMIT_ARGS: ' --packages org.apache.hadoop:hadoop-aws:3.1.1,mysql:mysql-connector-java:8.0.12,org.postgresql:postgresql:42.2.5\n",
       "        pyspark-shell'\n",
       "    SPARK_DIST_CLASSPATH:\n",
       "rootdir: /home/jovyan/work/basic\n",
       "timezone: UTC"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.project.engine().config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = dlf.project.engine().context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect some of the profile metadata loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profile', 'variables', 'engine', 'providers', 'resources', 'loggers']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect loaded metadata\n",
    "md = dlf.project.metadata()\n",
    "\n",
    "#print out the sections of the metadata\n",
    "list(md.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type: spark\n",
       "master: local[*]\n",
       "jobname:\n",
       "timezone: naive\n",
       "submit:\n",
       "    jars:\n",
       "    packages:\n",
       "    py-files:\n",
       "config:\n",
       "    spark.sql.autoBroadcastJoinThreshold: -1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md['engine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: hello\n",
       "b: hello world\n",
       "c: /bin/bash\n",
       "d: foo\n",
       "e: '2019-04-11 03:40:07'\n",
       "f: '2019-04-11'\n",
       "my_date_var: '2019-04-11'\n",
       "my_env_var: guest\n",
       "my_string_concat_var1: spark running at local[*]\n",
       "my_string_concat_var2: 'Hi There!: the current date is 2019-04-11'\n",
       "my_string_var: Hi There!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md['variables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type: spark\n",
       "master: local[*]\n",
       "jobname:\n",
       "timezone: naive\n",
       "submit:\n",
       "    jars:\n",
       "    packages:\n",
       "    py-files:\n",
       "config:\n",
       "    spark.sql.autoBroadcastJoinThreshold: -1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md['engine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root:\n",
       "    severity: info\n",
       "datalabframework:\n",
       "    name: dlf\n",
       "    stream:\n",
       "        severity:\n",
       "        enable:\n",
       "    stdio:\n",
       "        severity: notice\n",
       "        enable: true\n",
       "    file:\n",
       "        severity: notice\n",
       "        enable: true\n",
       "        path:\n",
       "    kafka:\n",
       "        severity: info\n",
       "        enable: false\n",
       "        hosts: kafka-node1:9092 kafka-node2:9092\n",
       "        topic: dlf"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md['loggers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect current project configuration\n",
    "\n",
    "You can inspect the current project configuration, by calling the `datalabframework.project.config` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function config in module datalabframework.project:\n",
      "\n",
      "config()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dlf.project.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "version: 0.7.0\n",
       "username: jovyan\n",
       "session_id: '0x7fe0ccc65c0b11e9'\n",
       "profile: default\n",
       "rootdir: /home/jovyan/work/basic\n",
       "script_path: main.ipynb\n",
       "dotenv_path: .env\n",
       "notebooks_files:\n",
       "  - main.ipynb\n",
       "  - versions.ipynb\n",
       "  - hello.ipynb\n",
       "  - main-Copy1.ipynb\n",
       "python_files:\n",
       "  - __main__.py\n",
       "metadata_files:\n",
       "  - metadata.yml\n",
       "repository:\n",
       "    type:\n",
       "    committer: ''\n",
       "    hash: 0\n",
       "    commit: 0\n",
       "    branch: ''\n",
       "    url: ''\n",
       "    name: ''\n",
       "    date: ''\n",
       "    clean: false"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.project.config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data resources are relative to the `rootpath`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.driver.host', '5de54992bc62')\n",
      "('spark.repl.local.jars', 'file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar')\n",
      "('spark.app.id', 'local-1554954088157')\n",
      "('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
      "('spark.jars', 'file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar')\n",
      "('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\n",
      "('spark.hadoop.fs.s3a.access.key', 'minio-username')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.submit.pyFiles', '/home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,/home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,/home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,/home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar')\n",
      "('spark.hadoop.fs.s3a.secret.key', 'minio-password')\n",
      "('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
      "('spark.driver.extraJavaOptions', '-Duser.timezone=UTC')\n",
      "('spark.driver.port', '38911')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.files', 'file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.hadoop.fs.s3a.endpoint', 'http://minio:9000')\n",
      "('spark.sql.session.timeZone', 'UTC')\n",
      "('spark.sql.autoBroadcastJoinThreshold', '-1')\n",
      "('spark.app.name', 'None')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SparkContext.getOrCreate().stop()\n",
    "#'spark://spark-master:7077'\n",
    "spark = SparkSession.builder\\\n",
    "            .master('local[*]') \\\n",
    "            .config('spark.hadoop.fs.s3a.endpoint','http://minio:9000')\\\n",
    "            .config('spark.hadoop.fs.s3a.impl','org.apache.hadoop.fs.s3a.S3AFileSystem')\\\n",
    "            .config('spark.hadoop.fs.s3a.path.style.access','true')\\\n",
    "            .config('spark.hadoop.fs.s3a.access.key','minio-username')\\\n",
    "            .config('spark.hadoop.fs.s3a.secret.key','minio-password')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "for i in spark.sparkContext.getConf().getAll():\n",
    "    print(i)\n",
    "\n",
    "df = spark.createDataFrame([(1,2), (3,4)], ['a', 'b'])\n",
    "\n",
    "# df.write.options(inferSchema=True, header=True).csv('file:///home/jovyan/work/basic/ohohpippo123.csv')\n",
    "# spark.read.options(inferSchema=True, header=True).csv('pippo123.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "df = spark.createDataFrame(((1,2), (3,4)))\n",
    "df.write.parquet('s3a://data/abc',mode='overwrite')\n",
    "\n",
    "df = spark.read.parquet('s3a://data/abc')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading packages:\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.postgresql:postgresql:42.2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datalabframework.project.Project at 0x7fdec84bc940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "dlf.project.load('prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = dlf.project.engine().context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(((1,2), (3,4)))\n",
    "df.write.parquet('s3a://data/abc',mode='overwrite')\n",
    "\n",
    "df = spark.read.parquet('s3a://data/abc')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hash': '0x9a37c2af26109088',\n",
       " 'url': 's3a://data/xyz',\n",
       " 'service': 'minio',\n",
       " 'format': 'parquet',\n",
       " 'host': 'minio',\n",
       " 'port': 9000,\n",
       " 'driver': None,\n",
       " 'database': None,\n",
       " 'schema': None,\n",
       " 'table': None,\n",
       " 'username': 'minio-username',\n",
       " 'password': 'minio-password',\n",
       " 'resource_path': 'xyz',\n",
       " 'provider_path': '/data',\n",
       " 'provider_alias': 'minio',\n",
       " 'resource_alias': 'xyz',\n",
       " 'cache': None,\n",
       " 'date_column': None,\n",
       " 'date_start': None,\n",
       " 'date_end': None,\n",
       " 'date_window': None,\n",
       " 'date_partition': None,\n",
       " 'update_column': None,\n",
       " 'hash_column': None,\n",
       " 'state_column': None,\n",
       " 'options': {},\n",
       " 'mapping': {}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = dlf.project.engine()\n",
    "\n",
    "md = dlf.project.resource('xyz', 'minio')\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "engine.save(df,md, mode='overwrite')\n",
    "df=engine.load(md)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "Data binding works with the metadata files. It's a good practice to declare the actual binding in the metadata and avoiding hardcoding the paths in the notebooks and python source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hash': '0x1586b5767f0c5518',\n",
       " 'url': '/home/jovyan/work/basic/data/ascombe.csv',\n",
       " 'service': 'file',\n",
       " 'format': 'csv',\n",
       " 'host': '127.0.0.1',\n",
       " 'port': None,\n",
       " 'driver': None,\n",
       " 'database': None,\n",
       " 'schema': None,\n",
       " 'table': None,\n",
       " 'username': None,\n",
       " 'password': None,\n",
       " 'resource_path': 'ascombe.csv',\n",
       " 'provider_path': '/home/jovyan/work/basic/data',\n",
       " 'provider_alias': 'localfs',\n",
       " 'resource_alias': 'ascombe',\n",
       " 'cache': None,\n",
       " 'date_column': None,\n",
       " 'date_start': None,\n",
       " 'date_end': None,\n",
       " 'date_window': None,\n",
       " 'date_partition': None,\n",
       " 'update_column': None,\n",
       " 'hash_column': None,\n",
       " 'state_column': None,\n",
       " 'options': {'header': True, 'inferSchema': True},\n",
       " 'mapping': {}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md =dlf.project.resource('ascombe')\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules: Engines\n",
    "\n",
    "This submodules will allow you to start a context, from the configuration described in the metadata. It also provide, basic load/store data functions according to the aliases defined in the configuration.\n",
    "\n",
    "Let's start by listing the aliases and the configuration of the engines declared in `metadata.yml`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Context: Spark__  \n",
    "Let's start the engine session, by selecting a spark context from the list. Your can have many spark contexts declared, for instance for single node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type: spark\n",
       "name: prod\n",
       "version: 2.4.0\n",
       "info:\n",
       "    python_version: 3.6.8\n",
       "    hadoop_version: 3.1.1\n",
       "    hadoop_detect: spark\n",
       "    hadoop_home: /opt/hadoop\n",
       "    spark_home: /opt/spark\n",
       "    spark_classpath:\n",
       "      - /opt/spark/jars/*\n",
       "      - /opt/hadoop/etc/hadoop\n",
       "      - /opt/hadoop/share/hadoop/common/lib/*\n",
       "      - /opt/hadoop/share/hadoop/common/*\n",
       "      - /opt/hadoop/share/hadoop/hdfs\n",
       "      - /opt/hadoop/share/hadoop/hdfs/lib/*\n",
       "      - /opt/hadoop/share/hadoop/hdfs/*\n",
       "      - /opt/hadoop/share/hadoop/mapreduce/lib/*\n",
       "      - /opt/hadoop/share/hadoop/mapreduce/*\n",
       "      - /opt/hadoop/share/hadoop/yarn\n",
       "      - /opt/hadoop/share/hadoop/yarn/lib/*\n",
       "      - /opt/hadoop/share/hadoop/yarn/*\n",
       "    spark_classpath_source: /opt/spark/conf/spark-env.sh\n",
       "config:\n",
       "    spark.driver.host: 5de54992bc62\n",
       "    spark.repl.local.jars: file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.driver.port: '33961'\n",
       "    spark.hadoop.fs.s3a.path.style.access: 'true'\n",
       "    spark.jars: file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.executor.extraJavaOptions: -Duser.timezone=UTC\n",
       "    spark.hadoop.fs.s3a.access.key: minio-username\n",
       "    spark.executor.id: driver\n",
       "    spark.submit.pyFiles: /home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,/home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,/home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,/home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.hadoop.fs.s3a.secret.key: minio-password\n",
       "    spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "    spark.driver.extraJavaOptions: -Duser.timezone=UTC\n",
       "    spark.rdd.compress: 'True'\n",
       "    spark.master: spark://spark-master:7077\n",
       "    spark.files: file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.1.1.jar,file:///home/jovyan/.ivy2/jars/mysql_mysql-connector-java-8.0.12.jar,file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar,file:///home/jovyan/.ivy2/jars/com.google.protobuf_protobuf-java-2.6.0.jar\n",
       "    spark.serializer.objectStreamReset: '100'\n",
       "    spark.app.id: app-20190411034147-0120\n",
       "    spark.submit.deployMode: client\n",
       "    spark.hadoop.fs.s3a.endpoint: http://minio:9000\n",
       "    spark.sql.session.timeZone: UTC\n",
       "    spark.sql.autoBroadcastJoinThreshold: '-1'\n",
       "    spark.app.name: None\n",
       "    spark.ui.showConsoleProgress: 'true'\n",
       "env:\n",
       "    SPARK_HOME: /opt/spark\n",
       "    HADOOP_HOME: /opt/hadoop\n",
       "    JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64\n",
       "    PYTHONPATH: /opt/spark/python:/opt/spark/python/lib/py4j-0.10.7-src.zip\n",
       "    PYSPARK_SUBMIT_ARGS: ' --packages org.apache.hadoop:hadoop-aws:3.1.1,mysql:mysql-connector-java:8.0.12,org.postgresql:postgresql:42.2.5\n",
       "        pyspark-shell'\n",
       "    SPARK_DIST_CLASSPATH:\n",
       "rootdir: /home/jovyan/work/basic\n",
       "timezone: UTC"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = dlf.project.engine()\n",
    "engine.config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can quickly inspect the properties of the context by calling the `info()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the `context` method, you access the Spark SQL Context directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = engine.context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's read the csv data again, this time using the spark context. First using the engine `write` utility, then directly using the spark context and the `dlf.data.path` function to localize our labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - load_with_pandas - Fallback dataframe reader - {}\n",
      "+---+----+----+----+----+----+----+---+----+\n",
      "|_c0| _c1| _c2| _c3| _c4| _c5| _c6|_c7| _c8|\n",
      "+---+----+----+----+----+----+----+---+----+\n",
      "|idx|  Ix|  Iy| IIx| IIy|IIIx|IIIy|IVx| IVy|\n",
      "|  0|10.0|8.04|10.0|9.14|10.0|7.46|8.0|6.58|\n",
      "+---+----+----+----+----+----+----+---+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read using the engine utility (directly using the load function)\n",
    "# load/save option from the metadata.yml files\n",
    "df = engine.load('ascombe')\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hash': '0x1586b5767f0c5518',\n",
       " 'url': '/home/jovyan/work/basic/data/ascombe.csv',\n",
       " 'service': 'file',\n",
       " 'format': 'csv',\n",
       " 'host': '127.0.0.1',\n",
       " 'port': None,\n",
       " 'driver': None,\n",
       " 'database': None,\n",
       " 'schema': None,\n",
       " 'table': None,\n",
       " 'username': None,\n",
       " 'password': None,\n",
       " 'resource_path': 'ascombe.csv',\n",
       " 'provider_path': '/home/jovyan/work/basic/data',\n",
       " 'provider_alias': 'localfs',\n",
       " 'resource_alias': 'ascombe',\n",
       " 'cache': None,\n",
       " 'date_column': None,\n",
       " 'date_start': None,\n",
       " 'date_end': None,\n",
       " 'date_window': None,\n",
       " 'date_partition': None,\n",
       " 'update_column': None,\n",
       " 'hash_column': None,\n",
       " 'state_column': None,\n",
       " 'options': {'header': True, 'inferSchema': True},\n",
       " 'mapping': {}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read using the engine utility (directly using the load function, option inline)\n",
    "md = dlf.project.resource('ascombe')\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf\n",
    "dlf.project.load('default')\n",
    "engine = dlf.project.engine()\n",
    "spark  = engine.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = engine.load('ascombe', header=True)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save(df,'correlation', mode='overwrite')\n",
    "engine.load('correlation').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save(df,'correlation', mode='overwrite', header=False)\n",
    "engine.load('correlation', header=False).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save(df,'correlation', mode='overwrite', header=True)\n",
    "engine.load('correlation', header=True).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf\n",
    "dlf.project.load('prod')\n",
    "engine = dlf.project.engine()\n",
    "\n",
    "df = engine.load('ascombe', header=True)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save(df,'correlation', mode='overwrite')\n",
    "engine.load('correlation').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save(df,'correlation', mode='overwrite', header=False)\n",
    "engine.load('correlation', header=False).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save(df,'correlation', mode='overwrite', header=True)\n",
    "engine.load('correlation', header=True).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read using the engine utility (also from resource metadata)\n",
    "md =dlf.project.resource('ascombe')\n",
    "df = engine.load(md, inferSchema=True, header=True)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate the correlation for each set I,II, III, IV between the `x` and `y` columns and save the result on an separate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "for s in ['I', 'II', 'III', 'IV']:\n",
    "    va = VectorAssembler(inputCols=[s+'x', s+'y'], outputCol=s)\n",
    "    df = va.transform(df)\n",
    "    df = df.drop(s+'x', s+'y')\n",
    "    \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assembling the dataframe into four sets of 2D vectors, let's calculate the pearson correlation for each set. In the case the the ascombe sets, all sets should have the same pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = engine.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "corr = {}\n",
    "cols = ['I', 'II', 'III', 'IV']\n",
    "\n",
    "# calculate pearson correlations\n",
    "for s in cols:\n",
    "    corr[s] = Correlation.corr(df, s, 'pearson').collect()[0][0][0,1].item()\n",
    "\n",
    "# declare schema\n",
    "from pyspark.sql.types import StructType, StructField, FloatType\n",
    "schema = StructType([StructField(s, FloatType(), True) for s in cols])\n",
    "\n",
    "# create output dataframe\n",
    "corr_df = spark.createDataFrame(data=[corr], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "corr_df.select([f.round(f.avg(c), 3).alias(c) for c in cols]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results. It's a very small data frame, however Spark when saving  csv format files, assumes large data sets and partitions the files inside an object (a directory) with the name of the target file. See below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save(corr_df,'correlation', mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roundtrip, reading it back to check all went fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.load('correlation', header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access data from HDFS\n",
    "\n",
    "We can override the default resource provider, \n",
    "by explicitely passing a different provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = dlf.project.resource('correlation', 'hdfs')\n",
    "engine.save(corr_df,md, header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.load('correlation', 'hdfs', header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access data from Minio\n",
    "\n",
    "Minio is an object storage server released under Apache License v2.0. It is compatible with Amazon S3 cloud storage service.   \n",
    "It is best suited for storing unstructured data such as photos, videos, log files, backups and container / VM images.   \n",
    "Size of an object can range from a few KBs to a maximum of 5TB. More info at https://github.com/minio/minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = dlf.project.resource('correlation', 'minio')\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = dlf.project.resource('correlation', 'minio')\n",
    "engine.save(corr_df,md, header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.load('correlation', 'minio', header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect data objects given a provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading packages:\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.postgresql:postgresql:42.2.5\n"
     ]
    }
   ],
   "source": [
    "dlf.project.load('prod')\n",
    "engine = dlf.project.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.ipynb_checkpoints</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>correlation.csv</td>\n",
       "      <td>FILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ascombe.csv</td>\n",
       "      <td>FILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prod</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name       type\n",
       "0  .ipynb_checkpoints  DIRECTORY\n",
       "1     correlation.csv       FILE\n",
       "2         ascombe.csv       FILE\n",
       "3                prod  DIRECTORY"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list('localfs').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>etl</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raw</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reports</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ml</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name       type\n",
       "0      etl  DIRECTORY\n",
       "1      raw  DIRECTORY\n",
       "2  reports  DIRECTORY\n",
       "3       ml  DIRECTORY"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list(path='prod', provider='localfs').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>correlation.csv</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name       type\n",
       "0  correlation.csv  DIRECTORY"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list('hdfs').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_temporary</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name       type\n",
       "0  _temporary  DIRECTORY"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list(path='correlation.csv', provider='hdfs').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, type]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list('minio').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Department</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Person</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OnsiteCourse</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OnlineCourse</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>StudentGrade</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CourseInstructor</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Course</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OfficeAssignment</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name        type\n",
       "0        Department  BASE TABLE\n",
       "1            Person  BASE TABLE\n",
       "2      OnsiteCourse  BASE TABLE\n",
       "3      OnlineCourse  BASE TABLE\n",
       "4      StudentGrade  BASE TABLE\n",
       "5  CourseInstructor  BASE TABLE\n",
       "6            Course  BASE TABLE\n",
       "7  OfficeAssignment  BASE TABLE"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list('school').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TABLE_CATALOG</th>\n",
       "      <th>TABLE_SCHEMA</th>\n",
       "      <th>TABLE_NAME</th>\n",
       "      <th>TABLE_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>School</td>\n",
       "      <td>dbo</td>\n",
       "      <td>Department</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>School</td>\n",
       "      <td>dbo</td>\n",
       "      <td>Person</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>School</td>\n",
       "      <td>dbo</td>\n",
       "      <td>OnsiteCourse</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>School</td>\n",
       "      <td>dbo</td>\n",
       "      <td>OnlineCourse</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>School</td>\n",
       "      <td>dbo</td>\n",
       "      <td>StudentGrade</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>School</td>\n",
       "      <td>dbo</td>\n",
       "      <td>CourseInstructor</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>School</td>\n",
       "      <td>dbo</td>\n",
       "      <td>Course</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>School</td>\n",
       "      <td>dbo</td>\n",
       "      <td>OfficeAssignment</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TABLE_CATALOG TABLE_SCHEMA        TABLE_NAME  TABLE_TYPE\n",
       "0        School          dbo        Department  BASE TABLE\n",
       "1        School          dbo            Person  BASE TABLE\n",
       "2        School          dbo      OnsiteCourse  BASE TABLE\n",
       "3        School          dbo      OnlineCourse  BASE TABLE\n",
       "4        School          dbo      StudentGrade  BASE TABLE\n",
       "5        School          dbo  CourseInstructor  BASE TABLE\n",
       "6        School          dbo            Course  BASE TABLE\n",
       "7        School          dbo  OfficeAssignment  BASE TABLE"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.load('SELECT * FROM INFORMATION_SCHEMA.TABLES', 'school').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actor</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actor_info</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>address</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>category</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>city</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>country</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>customer</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>customer_list</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>film</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>film_actor</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>film_category</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>film_list</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>film_text</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>inventory</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>language</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nicer_but_slower_film_list</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>payment</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rental</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sales_by_film_category</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sales_by_store</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>staff</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>staff_list</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>store</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name        type\n",
       "0                        actor  BASE TABLE\n",
       "1                   actor_info        VIEW\n",
       "2                      address  BASE TABLE\n",
       "3                     category  BASE TABLE\n",
       "4                         city  BASE TABLE\n",
       "5                      country  BASE TABLE\n",
       "6                     customer  BASE TABLE\n",
       "7                customer_list        VIEW\n",
       "8                         film  BASE TABLE\n",
       "9                   film_actor  BASE TABLE\n",
       "10               film_category  BASE TABLE\n",
       "11                   film_list        VIEW\n",
       "12                   film_text  BASE TABLE\n",
       "13                   inventory  BASE TABLE\n",
       "14                    language  BASE TABLE\n",
       "15  nicer_but_slower_film_list        VIEW\n",
       "16                     payment  BASE TABLE\n",
       "17                      rental  BASE TABLE\n",
       "18      sales_by_film_category        VIEW\n",
       "19              sales_by_store        VIEW\n",
       "20                       staff  BASE TABLE\n",
       "21                  staff_list        VIEW\n",
       "22                       store  BASE TABLE"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list('sakila').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actor_info</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer_list</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film_list</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nicer_but_slower_film_list</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>film</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>inventory</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>staff</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>payment_p2007_05</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>payment_p2007_06</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sales_by_film_category</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>category</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>payment_p2007_04</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>actor</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>language</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>country</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>film_actor</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>payment</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>store</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>film_category</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>payment_p2007_03</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rental</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>payment_p2007_01</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sales_by_store</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>staff_list</td>\n",
       "      <td>VIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>payment_p2007_02</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>address</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>city</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>customer</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name        type\n",
       "0                   actor_info        VIEW\n",
       "1                customer_list        VIEW\n",
       "2                    film_list        VIEW\n",
       "3   nicer_but_slower_film_list        VIEW\n",
       "4                         film  BASE TABLE\n",
       "5                    inventory  BASE TABLE\n",
       "6                        staff  BASE TABLE\n",
       "7             payment_p2007_05  BASE TABLE\n",
       "8             payment_p2007_06  BASE TABLE\n",
       "9       sales_by_film_category        VIEW\n",
       "10                    category  BASE TABLE\n",
       "11            payment_p2007_04  BASE TABLE\n",
       "12                       actor  BASE TABLE\n",
       "13                    language  BASE TABLE\n",
       "14                     country  BASE TABLE\n",
       "15                  film_actor  BASE TABLE\n",
       "16                     payment  BASE TABLE\n",
       "17                       store  BASE TABLE\n",
       "18               film_category  BASE TABLE\n",
       "19            payment_p2007_03  BASE TABLE\n",
       "20                      rental  BASE TABLE\n",
       "21            payment_p2007_01  BASE TABLE\n",
       "22              sales_by_store        VIEW\n",
       "23                  staff_list        VIEW\n",
       "24            payment_p2007_02  BASE TABLE\n",
       "25                     address  BASE TABLE\n",
       "26                        city  BASE TABLE\n",
       "27                    customer  BASE TABLE"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list('pagila').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>actor</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>address</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>category</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>city</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>country</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>customer</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>film</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>film_actor</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>film_category</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>inventory</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>language</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>payment</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>payment_p2007_01</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>payment_p2007_02</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>payment_p2007_03</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>payment_p2007_04</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>payment_p2007_05</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>payment_p2007_06</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rental</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>staff</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>store</td>\n",
       "      <td>BASE TABLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name        type\n",
       "12             actor  BASE TABLE\n",
       "25           address  BASE TABLE\n",
       "10          category  BASE TABLE\n",
       "26              city  BASE TABLE\n",
       "14           country  BASE TABLE\n",
       "27          customer  BASE TABLE\n",
       "4               film  BASE TABLE\n",
       "15        film_actor  BASE TABLE\n",
       "18     film_category  BASE TABLE\n",
       "5          inventory  BASE TABLE\n",
       "13          language  BASE TABLE\n",
       "16           payment  BASE TABLE\n",
       "21  payment_p2007_01  BASE TABLE\n",
       "24  payment_p2007_02  BASE TABLE\n",
       "19  payment_p2007_03  BASE TABLE\n",
       "11  payment_p2007_04  BASE TABLE\n",
       "7   payment_p2007_05  BASE TABLE\n",
       "8   payment_p2007_06  BASE TABLE\n",
       "20            rental  BASE TABLE\n",
       "6              staff  BASE TABLE\n",
       "17             store  BASE TABLE"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = engine.list('pagila').toPandas()\n",
    "df[df['type']=='BASE TABLE'].sort_values('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_name</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABNEY</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADAM</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADAMS</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALEXANDER</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALLARD</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ALLEN</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ALVAREZ</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANDERSON</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANDREW</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ANDREWS</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AQUINO</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ARCE</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ARCHULETA</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ARMSTRONG</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ARNOLD</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ARSENAULT</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ARTIS</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ASHCRAFT</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ASHER</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AUSTIN</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    last_name  amount\n",
       "0       ABNEY      21\n",
       "1        ADAM      28\n",
       "2       ADAMS      27\n",
       "3   ALEXANDER      27\n",
       "4      ALLARD      32\n",
       "5       ALLEN      31\n",
       "6     ALVAREZ      27\n",
       "7    ANDERSON      24\n",
       "8      ANDREW      25\n",
       "9     ANDREWS      23\n",
       "10     AQUINO      20\n",
       "11       ARCE      35\n",
       "12  ARCHULETA      30\n",
       "13  ARMSTRONG      25\n",
       "14     ARNOLD      26\n",
       "15  ARSENAULT      27\n",
       "16      ARTIS      23\n",
       "17   ASHCRAFT      23\n",
       "18      ASHER      24\n",
       "19     AUSTIN      35"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT c.last_name,\n",
    "             COUNT(p.amount) AS amount\n",
    "    FROM customer c\n",
    "    LEFT JOIN payment p\n",
    "        ON c.customer_id = p.customer_id\n",
    "    WHERE c.last_name like 'A%'\n",
    "    GROUP BY  c.last_name\n",
    "    ORDER BY  c.last_name ASC;\n",
    "\"\"\"\n",
    "engine.load(query, 'pagila').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules: Export\n",
    "\n",
    "This submodules will allow you to export cells and import them in other notebooks as python packages. Check the notebook [versions.ipynb](versions.ipynb), where you will see how to export the notebook, then follow the code here below to check it really works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf\n",
    "dlf.project.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hello import python_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world: python 3.6.3\n"
     ]
    }
   ],
   "source": [
    "python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules: Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading packages:\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.postgresql:postgresql:42.2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "root:\n",
       "    severity: info\n",
       "datalabframework:\n",
       "    name: dlf\n",
       "    stream:\n",
       "        severity:\n",
       "        enable:\n",
       "    stdio:\n",
       "        severity: notice\n",
       "        enable: true\n",
       "    file:\n",
       "        severity: notice\n",
       "        enable: true\n",
       "        path:\n",
       "    kafka:\n",
       "        severity: info\n",
       "        enable: false\n",
       "        hosts: kafka-node1:9092 kafka-node2:9092\n",
       "        topic: dlf"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "import datalabframework.logging as log\n",
    "\n",
    "dlf.project.load(factory_defaults=False)\n",
    "dlf.project.metadata()['loggers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR - run_code - test - {}\n",
      "WARNING - run_code - pipppo - {'a': 42}\n",
      "NOTICE - run_code - data - {'a': 42}\n"
     ]
    }
   ],
   "source": [
    "log.error('test')\n",
    "log.warning('pipppo', extra={'a':42})\n",
    "log.notice({'a':42})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to a database\n",
    "\n",
    "From host:\n",
    "`docker exec -it 23fc6098cbdf psql --username=postgres --dbname=pagila `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.env'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.files.get_dotenv_path(dlf.paths.rootdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading packages:\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.postgresql:postgresql:42.2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datalabframework.project.Project at 0x7fa8b7d51160>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "dlf.project.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalabframework.metadata import load as get_project_metadata\n",
    "from datalabframework import files, paths\n",
    "\n",
    "from datalabframework._utils import Singleton, YamlDict, repo_data, to_ordered_dict, python_version, relpath, abspath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'profile': 'default',\n",
       " 'variables': {'a': 'hello',\n",
       "  'b': 'hello world',\n",
       "  'c': '/bin/bash',\n",
       "  'd': 'foo',\n",
       "  'e': '2019-04-12 03:33:32',\n",
       "  'f': '2019-04-12',\n",
       "  'my_date_var': '2019-04-12',\n",
       "  'my_env_var': 'guest',\n",
       "  'my_string_concat_var1': 'spark running at local[*]',\n",
       "  'my_string_concat_var2': 'Hi There!: the current date is 2019-04-12',\n",
       "  'my_string_var': 'Hi There!'},\n",
       " 'engine': {'type': 'spark',\n",
       "  'master': 'local[*]',\n",
       "  'jobname': None,\n",
       "  'timezone': 'naive',\n",
       "  'submit': {'jars': None, 'packages': None, 'py-files': None},\n",
       "  'config': {'spark.sql.autoBroadcastJoinThreshold': -1}},\n",
       " 'providers': {'minio': {'username': 'minio-username',\n",
       "   'path': '/data',\n",
       "   'service': 'minio',\n",
       "   'hostname': 'minio',\n",
       "   'password': 'minio-password',\n",
       "   'format': 'parquet'},\n",
       "  'localfs': {'path': 'data', 'format': 'csv', 'service': 'file'},\n",
       "  'pagila': {'username': 'postgres',\n",
       "   'path': 'pagila',\n",
       "   'service': 'postgres',\n",
       "   'hostname': 'postgres',\n",
       "   'password': 'postgres',\n",
       "   'format': 'jdbc'},\n",
       "  'school': {'username': 'sa',\n",
       "   'path': 'School',\n",
       "   'service': 'mssql',\n",
       "   'hostname': 'mssql',\n",
       "   'password': 'Passw0rd',\n",
       "   'format': 'jdbc'},\n",
       "  'sakila': {'username': 'sakila',\n",
       "   'path': 'sakila',\n",
       "   'service': 'mysql',\n",
       "   'hostname': 'mysql',\n",
       "   'password': 'sakila',\n",
       "   'format': 'jdbc'},\n",
       "  'hdfs': {'path': '/',\n",
       "   'service': 'hdfs',\n",
       "   'hostname': 'hdfs-namenode',\n",
       "   'format': 'parquet'}},\n",
       " 'resources': {'correlation': {'path': 'correlation.csv',\n",
       "   'provider': 'localfs'},\n",
       "  'ascombe': {'options': {'header': True, 'inferSchema': True},\n",
       "   'provider': 'localfs',\n",
       "   'path': 'ascombe.csv'}},\n",
       " 'loggers': {'root': {'severity': 'info'},\n",
       "  'datalabframework': {'name': 'dlf',\n",
       "   'stream': {'severity': None, 'enable': None},\n",
       "   'stdio': {'severity': 'notice', 'enable': True},\n",
       "   'file': {'severity': 'notice', 'enable': True, 'path': None},\n",
       "   'kafka': {'severity': 'info',\n",
       "    'enable': False,\n",
       "    'hosts': 'kafka-node1:9092 kafka-node2:9092',\n",
       "    'topic': 'dlf'}}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_project_metadata(\n",
    "                'default',\n",
    "                abspath(files.get_metadata_files(paths.rootdir()), paths.rootdir()),\n",
    "                abspath(dlf.files.get_dotenv_path(dlf.paths.rootdir()), paths.rootdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading packages:\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.postgresql:postgresql:42.2.5\n"
     ]
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "dlf.project.load('prod')\n",
    "\n",
    "engine = dlf.project.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hash': '0x64af7cc8c77c8fba',\n",
       " 'url': 'jdbc:postgresql://postgres:5432/pagila',\n",
       " 'service': 'postgres',\n",
       " 'format': 'jdbc',\n",
       " 'host': 'postgres',\n",
       " 'port': 5432,\n",
       " 'driver': 'org.postgresql.Driver',\n",
       " 'database': 'pagila',\n",
       " 'schema': 'public',\n",
       " 'table': 'staff',\n",
       " 'username': 'postgres',\n",
       " 'password': 'postgres',\n",
       " 'resource_path': 'staff',\n",
       " 'provider_path': 'pagila',\n",
       " 'provider_alias': 'pagila',\n",
       " 'resource_alias': 'staff',\n",
       " 'cache': None,\n",
       " 'date_column': None,\n",
       " 'date_start': None,\n",
       " 'date_end': None,\n",
       " 'date_window': None,\n",
       " 'date_partition': None,\n",
       " 'update_column': None,\n",
       " 'hash_column': None,\n",
       " 'state_column': None,\n",
       " 'options': {},\n",
       " 'mapping': {}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = dlf.project.resource('staff', 'pagila')\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mike</td>\n",
       "      <td>Hillyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jon</td>\n",
       "      <td>Stephens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name last_name\n",
       "0       Mike   Hillyer\n",
       "1        Jon  Stephens"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = engine.load('staff', 'pagila')\n",
    "df.select('first_name', 'last_name').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_sales</th>\n",
       "      <th>last_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33927.040000000000000000</td>\n",
       "      <td>Stephens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33489.470000000000000000</td>\n",
       "      <td>Hillyer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                total_sales last_name\n",
       "0  33927.040000000000000000  Stephens\n",
       "1  33489.470000000000000000   Hillyer"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use JOIN to display the total amount rung up by each staff member\n",
    "# use tables 'staff' and 'payment'\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT SUM(p.amount) as total_sales, s.last_name\n",
    "    FROM payment p \n",
    "    INNER JOIN staff s ON p.staff_id = s.staff_id GROUP BY s.last_name\n",
    "    \"\"\"\n",
    "df = engine.load(query, 'pagila')\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0  16049"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = dlf.project.resource('SELECT COUNT(1) FRom payment', 'pagila')\n",
    "engine.load(md).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hash': '0x253970984be15485',\n",
       " 'url': 'jdbc:postgresql://postgres:5432/pagila',\n",
       " 'service': 'postgres',\n",
       " 'format': 'jdbc',\n",
       " 'host': 'postgres',\n",
       " 'port': 5432,\n",
       " 'driver': 'org.postgresql.Driver',\n",
       " 'database': 'pagila',\n",
       " 'schema': 'public',\n",
       " 'table': \"( SEleCT * FROM information_schema.tables WHERE table_type = 'BASE TABLE' AND table_schema = 'public' ORDER BY table_schema,table_name ) as _query\",\n",
       " 'username': 'postgres',\n",
       " 'password': 'postgres',\n",
       " 'resource_path': \"\\n    SEleCT *\\n    FROM information_schema.tables\\n    WHERE \\n        table_type = 'BASE TABLE' AND\\n        table_schema = 'public'\\n    ORDER BY table_schema,table_name\\n    \",\n",
       " 'provider_path': 'pagila',\n",
       " 'provider_alias': 'pagila',\n",
       " 'resource_alias': \"\\n    SEleCT *\\n    FROM information_schema.tables\\n    WHERE \\n        table_type = 'BASE TABLE' AND\\n        table_schema = 'public'\\n    ORDER BY table_schema,table_name\\n    \",\n",
       " 'cache': None,\n",
       " 'date_column': None,\n",
       " 'date_start': None,\n",
       " 'date_end': None,\n",
       " 'date_window': None,\n",
       " 'date_partition': None,\n",
       " 'update_column': None,\n",
       " 'hash_column': None,\n",
       " 'state_column': None,\n",
       " 'options': {},\n",
       " 'mapping': {}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SEleCT *\n",
    "    FROM information_schema.tables\n",
    "    WHERE \n",
    "        table_type = 'BASE TABLE' AND\n",
    "        table_schema = 'public'\n",
    "    ORDER BY table_schema,table_name\n",
    "    \"\"\"\n",
    "md = dlf.project.resource(query, 'pagila')\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_catalog</th>\n",
       "      <th>table_schema</th>\n",
       "      <th>table_name</th>\n",
       "      <th>table_type</th>\n",
       "      <th>self_referencing_column_name</th>\n",
       "      <th>reference_generation</th>\n",
       "      <th>user_defined_type_catalog</th>\n",
       "      <th>user_defined_type_schema</th>\n",
       "      <th>user_defined_type_name</th>\n",
       "      <th>is_insertable_into</th>\n",
       "      <th>is_typed</th>\n",
       "      <th>commit_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>actor</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>address</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>category</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>city</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>country</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>customer</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>film</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>film_actor</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>film_category</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>inventory</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>language</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>payment</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>payment_p2007_01</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>payment_p2007_02</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>payment_p2007_03</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>payment_p2007_04</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>payment_p2007_05</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>payment_p2007_06</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>rental</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>staff</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pagila</td>\n",
       "      <td>public</td>\n",
       "      <td>store</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   table_catalog table_schema        table_name  table_type  \\\n",
       "0         pagila       public             actor  BASE TABLE   \n",
       "1         pagila       public           address  BASE TABLE   \n",
       "2         pagila       public          category  BASE TABLE   \n",
       "3         pagila       public              city  BASE TABLE   \n",
       "4         pagila       public           country  BASE TABLE   \n",
       "5         pagila       public          customer  BASE TABLE   \n",
       "6         pagila       public              film  BASE TABLE   \n",
       "7         pagila       public        film_actor  BASE TABLE   \n",
       "8         pagila       public     film_category  BASE TABLE   \n",
       "9         pagila       public         inventory  BASE TABLE   \n",
       "10        pagila       public          language  BASE TABLE   \n",
       "11        pagila       public           payment  BASE TABLE   \n",
       "12        pagila       public  payment_p2007_01  BASE TABLE   \n",
       "13        pagila       public  payment_p2007_02  BASE TABLE   \n",
       "14        pagila       public  payment_p2007_03  BASE TABLE   \n",
       "15        pagila       public  payment_p2007_04  BASE TABLE   \n",
       "16        pagila       public  payment_p2007_05  BASE TABLE   \n",
       "17        pagila       public  payment_p2007_06  BASE TABLE   \n",
       "18        pagila       public            rental  BASE TABLE   \n",
       "19        pagila       public             staff  BASE TABLE   \n",
       "20        pagila       public             store  BASE TABLE   \n",
       "\n",
       "   self_referencing_column_name reference_generation  \\\n",
       "0                          None                 None   \n",
       "1                          None                 None   \n",
       "2                          None                 None   \n",
       "3                          None                 None   \n",
       "4                          None                 None   \n",
       "5                          None                 None   \n",
       "6                          None                 None   \n",
       "7                          None                 None   \n",
       "8                          None                 None   \n",
       "9                          None                 None   \n",
       "10                         None                 None   \n",
       "11                         None                 None   \n",
       "12                         None                 None   \n",
       "13                         None                 None   \n",
       "14                         None                 None   \n",
       "15                         None                 None   \n",
       "16                         None                 None   \n",
       "17                         None                 None   \n",
       "18                         None                 None   \n",
       "19                         None                 None   \n",
       "20                         None                 None   \n",
       "\n",
       "   user_defined_type_catalog user_defined_type_schema user_defined_type_name  \\\n",
       "0                       None                     None                   None   \n",
       "1                       None                     None                   None   \n",
       "2                       None                     None                   None   \n",
       "3                       None                     None                   None   \n",
       "4                       None                     None                   None   \n",
       "5                       None                     None                   None   \n",
       "6                       None                     None                   None   \n",
       "7                       None                     None                   None   \n",
       "8                       None                     None                   None   \n",
       "9                       None                     None                   None   \n",
       "10                      None                     None                   None   \n",
       "11                      None                     None                   None   \n",
       "12                      None                     None                   None   \n",
       "13                      None                     None                   None   \n",
       "14                      None                     None                   None   \n",
       "15                      None                     None                   None   \n",
       "16                      None                     None                   None   \n",
       "17                      None                     None                   None   \n",
       "18                      None                     None                   None   \n",
       "19                      None                     None                   None   \n",
       "20                      None                     None                   None   \n",
       "\n",
       "   is_insertable_into is_typed commit_action  \n",
       "0                 YES       NO          None  \n",
       "1                 YES       NO          None  \n",
       "2                 YES       NO          None  \n",
       "3                 YES       NO          None  \n",
       "4                 YES       NO          None  \n",
       "5                 YES       NO          None  \n",
       "6                 YES       NO          None  \n",
       "7                 YES       NO          None  \n",
       "8                 YES       NO          None  \n",
       "9                 YES       NO          None  \n",
       "10                YES       NO          None  \n",
       "11                YES       NO          None  \n",
       "12                YES       NO          None  \n",
       "13                YES       NO          None  \n",
       "14                YES       NO          None  \n",
       "15                YES       NO          None  \n",
       "16                YES       NO          None  \n",
       "17                YES       NO          None  \n",
       "18                YES       NO          None  \n",
       "19                YES       NO          None  \n",
       "20                YES       NO          None  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.load(md, 'pagila').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading packages:\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.postgresql:postgresql:42.2.5\n"
     ]
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "dlf.project.load('prod')\n",
    "\n",
    "engine = dlf.project.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(type,StringType,true)))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list('pagila').schema\n",
    "#.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, type: string]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list('localfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pippo'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger('pippo')\n",
    "logger.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, type: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list('minio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, type: string]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.list('hdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading packages:\n",
      "  -  org.apache.hadoop:hadoop-aws:3.1.1\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.postgresql:postgresql:42.2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datalabframework.project.Project at 0x7f55d15ca1d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalabframework as dlf\n",
    "import datalabframework.logging as log\n",
    "\n",
    "dlf.project.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTICE - run_code - data - {'rows': 23, 'time': 120}\n",
      "WARNING - run_code - my message - {}\n",
      "WARNING - run_code - this is another message - {'action': 'join', 'aaa': 42}\n"
     ]
    }
   ],
   "source": [
    "log.notice({'rows':23, 'time':120})\n",
    "log.warning('my message')\n",
    "log.warning('this is another message', extra={'action':'join', 'aaa':42})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTICE - run_code - data - {'python_version': '3.6.3', 'hadoop_home': '/opt/hadoop', 'hadoop_detect': 'spark', 'spark_classpath_source': '/opt/spark/conf/spark-env.sh', 'spark_home': '/opt/spark', 'spark_classpath': ['/opt/spark/jars/*', '/opt/hadoop/etc/hadoop', '/opt/hadoop/share/hadoop/common/lib/*', '/opt/hadoop/share/hadoop/common/*', '/opt/hadoop/share/hadoop/hdfs', '/opt/hadoop/share/hadoop/hdfs/lib/*', '/opt/hadoop/share/hadoop/hdfs/*', '/opt/hadoop/share/hadoop/mapreduce/lib/*', '/opt/hadoop/share/hadoop/mapreduce/*', '/opt/hadoop/share/hadoop/yarn', '/opt/hadoop/share/hadoop/yarn/lib/*', '/opt/hadoop/share/hadoop/yarn/*'], 'hadoop_version': '3.1.1'}\n"
     ]
    }
   ],
   "source": [
    "log.notice(dict(engine.config()['info']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
